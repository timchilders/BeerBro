{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#collaborative imports\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import dump\n",
    "from surprise import accuracy\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection.split import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv('../data/top_ratings.csv') #dataframe containing reviews for top beers\n",
    "df = df.dropna()\n",
    "df = df[~(df.score>5)]\n",
    "df['uid'] = df.groupby('username').ngroup()\n",
    "# user_df = df.pivot(index='uid',columns='beer_id',values='score')\n",
    "# user_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "class CollabRecommender():\n",
    "    '''\n",
    "    SVD Collaborative-Based Beer Recommender\n",
    "    '''\n",
    "    def __init__(self, df):\n",
    "        reader = Reader(rating_scale=(1,5))\n",
    "        self.data = Dataset.load_from_df(df[['uid','beer_id','score']],reader)\n",
    "        self.trainset, self.testset = train_test_split(self.data,test_size=.20)\n",
    "        self.model = self.load_obj('model')\n",
    "        try:\n",
    "            self.predictions = self.load_obj('predictions')\n",
    "        except:\n",
    "            print('Unable to load predictions, please use .predict() method.')\n",
    "        try:\n",
    "            self.model = self.load_obj('model')\n",
    "        except:\n",
    "            print('Unable to load model, please use the .fit() method.')\n",
    "\n",
    "    def fit(self, model=SVD, n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02):\n",
    "        self.model = model(n_factors=n_factors, n_epochs=n_epochs, lr_all=lr_all, reg_all=reg_all, verbose=True)\n",
    "        self.model.fit(self.trainset)\n",
    "        self.save_obj(self.model, 'model')\n",
    "    \n",
    "    def predict(self):\n",
    "        \n",
    "        print('Generating predictions...')\n",
    "        self.predictions = self.model.test(self.testset)\n",
    "    \n",
    "    def get_top_n(self, n=10):\n",
    "        '''\n",
    "        Return the top-N recommendation for each user from a set of predictions.\n",
    "        Input:\n",
    "            predictions(list of Prediction objects): The list of predictions, as\n",
    "                returned by the test method of an algorithm.\n",
    "            n(int): The number of recommendation to output for each user. Default\n",
    "                is 10.\n",
    "\n",
    "        Returns:\n",
    "        A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "            [(raw item id, rating estimation), ...] of size n.\n",
    "        '''\n",
    "\n",
    "        # First map the predictions to each user.\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in self.predictions:\n",
    "            top_n[uid].append((iid, est))\n",
    "\n",
    "        # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = user_ratings[:n]\n",
    "\n",
    "        return top_n\n",
    "    \n",
    "\n",
    "    #pickle helper functions\n",
    "    def save_obj(self,obj, name):\n",
    "        with open('../obj/'+ name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_obj(self,name):\n",
    "        with open('../obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)     \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "colab_r = CollabRecommender(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "predictions = colab_r.predict()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating predictions...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=4):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "precisions, recalls = precision_recall_at_k(colab_r.predictions,k=5,threshold=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7297250536157246\n",
      "0.6057569603504699\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "top_ratings = colab_r.get_top_n()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "top_ratings[39309]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(16909, 4.329816504640543),\n",
       " (10325, 4.291671178021917),\n",
       " (1577, 4.263637058932049),\n",
       " (18862, 4.195485977571795),\n",
       " (49472, 4.182998319583871),\n",
       " (705, 4.175620853459322),\n",
       " (3916, 4.137831205635079),\n",
       " (6646, 4.119690070148591),\n",
       " (30, 4.1029104887799095),\n",
       " (25852, 4.081892898481363)]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "sim_users = [170, 203,  87, 169, 190]\n",
    "beers={}\n",
    "for user in sim_users:\n",
    "    for item in top_ratings[user]:\n",
    "        if len(item)>0:\n",
    "            beers[item[0]]=item[1]\n",
    "\n",
    "beers\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1372: 4.062112052778878, 7463: 4.1464328095279495, 22343: 3.8146801284069456}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "beers = dict(sorted(beers.items(),key=lambda item: item[1], reverse=True))\n",
    "beers"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{7463: 4.1464328095279495, 1372: 4.062112052778878, 22343: 3.8146801284069456}"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "max(beers,key=beers.get)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7463"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_Iu(uid):\n",
    "    \"\"\" return the number of items rated by given user\n",
    "    args: \n",
    "      uid: the id of the user\n",
    "    returns: \n",
    "      the number of items rated by the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0\n",
    "    \n",
    "def get_Ui(iid):\n",
    "    \"\"\" return number of users that have rated given item\n",
    "    args:\n",
    "      iid: the raw id of the item\n",
    "    returns:\n",
    "      the number of users that have rated the item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "pred_df = pd.DataFrame(predictions, columns=['username', 'beer_id', 'rui', 'est', 'details'])\n",
    "pred_df['Iu'] = pred_df.username.apply(get_Iu)\n",
    "pred_df['Ui'] = pred_df.beer_id.apply(get_Ui)\n",
    "pred_df['err'] = abs(pred_df.est - df.rui)\n",
    "best_predictions = pred_df.sort_values(by='err')[:10]\n",
    "worst_predictions = pred_df.sort_values(by='err')[-10:]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f3d71ba50c6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beer_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rui'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'details'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Iu'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_Iu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ui'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeer_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_Ui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pu = colab_r.model.pu\n",
    "qi = colab_r.model.qi\n",
    "bu = colab_r.model.bu\n",
    "bi = colab_r.model.bi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "bi.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "avg=df.score.mean()\n",
    "pq = np.dot((bi[:,None]+qi),(bu[:,None]+pu).T)\n",
    "prediction = (avg+pq).T\n",
    "prediction.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(122278, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "prediction[prediction>5]=5\n",
    "prediction[prediction<0]=0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "top_recs = prediction.argsort()\n",
    "top_recs[0][:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  0, 629, 628, 626, 625, 624, 622, 616, 610, 605])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "df.iloc[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "beer_id      125646\n",
       "username    _dirty_\n",
       "text               \n",
       "score           4.5\n",
       "uid           68954\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "eec48ea81786c325bfab6147fce8b71cfe42370920a0568aac6dc72a8f882954"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}